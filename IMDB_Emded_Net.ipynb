{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray,dot\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import Callable,Dict\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S:\\\\S_ML\\\\S_Work\\\\DEEP_Learning'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"S:\\S_ML\\DataSets\\IMDB\\IMDB.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "count                                               50000     50000\n",
       "unique                                              49582         2\n",
       "top     Loved today's show!!! It was a variety and not...  negative\n",
       "freq                                                    5     25000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      "review       50000 non-null object\n",
      "sentiment    50000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['review'][0]  # first review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'][2]  # Third review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in data['review']:\n",
    "    reviews.append(r)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to create a list of values, given a predictor variable.\n",
    "def Create_list(data,col:str)->list:\n",
    "    L = []\n",
    "    \n",
    "    for s in data[col]:\n",
    "        L.append(s)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = Create_list(data,'sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to creates a vocabulary of distinct words, given a dataset.\n",
    "def Create_Vocabulary(input_:list)->set:\n",
    "    vocabulary = set()\n",
    "    \n",
    "    for review in input_:\n",
    "        token = review.split()\n",
    "        for word in token:\n",
    "            vocabulary.add(word)\n",
    "            \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocabulary = Create_Vocabulary(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438729"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates a dictionary of words. The vocabulary of words are mapped into the set of integers.\n",
    "# The input is a set of words.\n",
    "def Create_Mappings(S:set)->Dict[str,ndarray]:\n",
    "    Word_dict = dict()\n",
    "    \n",
    "    for num,word in enumerate(S):\n",
    "        Word_dict[word] = num\n",
    "        \n",
    "    return Word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2num = Create_Mappings(Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Create_list_num -> function recreates the reviews(the text data) into a list of integers.\n",
    "# The input is the text data stored as a list and the dictionary which stores the word mappings.\n",
    "def Create_list_num(input_:list,word2num:Dict[str,ndarray])->list:\n",
    "    Output = []\n",
    "    \n",
    "    for sentence in input_:\n",
    "        tokens = sentence.split()\n",
    "        num_list = []\n",
    "        for word in tokens:\n",
    "            try:\n",
    "                num_list.append(word2num[word])\n",
    "            except:\n",
    "                \"\"\n",
    "        Output.append(num_list)\n",
    "        \n",
    "    return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = Create_list_num(reviews,word2num) # input_ is stored as a list\n",
    "input_ = np.array(input_) # Converting the list into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[258537,\n",
       " 271110,\n",
       " 278553,\n",
       " 109657,\n",
       " 241080,\n",
       " 374026,\n",
       " 91722,\n",
       " 314818,\n",
       " 387265,\n",
       " 398421,\n",
       " 50919,\n",
       " 372338,\n",
       " 50919,\n",
       " 430722,\n",
       " 413381,\n",
       " 86234,\n",
       " 175185,\n",
       " 259130,\n",
       " 94076,\n",
       " 86234,\n",
       " 262148,\n",
       " 331952,\n",
       " 37615,\n",
       " 366692,\n",
       " 86805,\n",
       " 373475,\n",
       " 414054,\n",
       " 303536,\n",
       " 51949,\n",
       " 241080,\n",
       " 374026,\n",
       " 91722,\n",
       " 50253,\n",
       " 362321,\n",
       " 227097,\n",
       " 78147,\n",
       " 6585,\n",
       " 370317,\n",
       " 43409,\n",
       " 130367,\n",
       " 121367,\n",
       " 122477,\n",
       " 269723,\n",
       " 8900,\n",
       " 414054,\n",
       " 136313,\n",
       " 181147,\n",
       " 362519,\n",
       " 29362,\n",
       " 8900,\n",
       " 414054,\n",
       " 9215,\n",
       " 12171,\n",
       " 209591,\n",
       " 429620,\n",
       " 97095,\n",
       " 179838,\n",
       " 278723,\n",
       " 209217,\n",
       " 414054,\n",
       " 354327,\n",
       " 200067,\n",
       " 78901,\n",
       " 149510,\n",
       " 414054,\n",
       " 331357,\n",
       " 373475,\n",
       " 204194,\n",
       " 265068,\n",
       " 239971,\n",
       " 130367,\n",
       " 121367,\n",
       " 398421,\n",
       " 280382,\n",
       " 78147,\n",
       " 324760,\n",
       " 414054,\n",
       " 214598,\n",
       " 181147,\n",
       " 280382,\n",
       " 398421,\n",
       " 259130,\n",
       " 276202,\n",
       " 103882,\n",
       " 86234,\n",
       " 175844,\n",
       " 51949,\n",
       " 258537,\n",
       " 144381,\n",
       " 427879,\n",
       " 216277,\n",
       " 258150,\n",
       " 366692,\n",
       " 414054,\n",
       " 262971,\n",
       " 165887,\n",
       " 366692,\n",
       " 208745,\n",
       " 86234,\n",
       " 281777,\n",
       " 4557,\n",
       " 241080,\n",
       " 374026,\n",
       " 91722,\n",
       " 86805,\n",
       " 204783,\n",
       " 121931,\n",
       " 312819,\n",
       " 277089,\n",
       " 414054,\n",
       " 278553,\n",
       " 41741,\n",
       " 414054,\n",
       " 238288,\n",
       " 366692,\n",
       " 414054,\n",
       " 39264,\n",
       " 105853,\n",
       " 43308,\n",
       " 341964,\n",
       " 207220,\n",
       " 414054,\n",
       " 367320,\n",
       " 208937,\n",
       " 233252,\n",
       " 329002,\n",
       " 158923,\n",
       " 50836,\n",
       " 422085,\n",
       " 274883,\n",
       " 238997,\n",
       " 331220,\n",
       " 196132,\n",
       " 204721,\n",
       " 86234,\n",
       " 196132,\n",
       " 163034,\n",
       " 218281,\n",
       " 277089,\n",
       " 414054,\n",
       " 287877,\n",
       " 229124,\n",
       " 115014,\n",
       " 86234,\n",
       " 428889,\n",
       " 86234,\n",
       " 414054,\n",
       " 406830,\n",
       " 71498,\n",
       " 366692,\n",
       " 43591,\n",
       " 258280,\n",
       " 277089,\n",
       " 382416,\n",
       " 268986,\n",
       " 58006,\n",
       " 113525,\n",
       " 345509,\n",
       " 362321,\n",
       " 77399,\n",
       " 78147,\n",
       " 431290]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to recreate the Sentiments as a list of zeros and ones. Positive review is mapped to 1 and a negative review is\n",
    "# mapped to zero.\n",
    "\n",
    "def Create_list_sentiments(data:list)->list:\n",
    "    output = []\n",
    "    \n",
    "    for sentiment in data:\n",
    "        if sentiment == \"positive\":\n",
    "            output.append(1)\n",
    "        else:\n",
    "            output.append(0)\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ = Create_list_sentiments(sentiments)\n",
    "output_ = np.array(output_)  # Converting the list into numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(S:ndarray)->ndarray:  # Sigmoid activation.\n",
    "    return (1/(1+np.exp(-S)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid_der(S:ndarray)->ndarray:\n",
    "    return Sigmoid(S)*(1-Sigmoid(S))  # Derivative  of the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(S:ndarray)->ndarray:\n",
    "    return np.tanh(S)           # the hyperbolic tangent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_der(S:ndarray)->ndarray:      # the derivative of the tanh function.\n",
    "    return (1-np.power(np.tanh(S),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Relu(S:ndarray)->ndarray:   # The relu function.\n",
    "    return np.maximum(S,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Relu_der(S:ndarray)->ndarray:\n",
    "    der = S.copy()                 # The derivative of the relu function.\n",
    "    \n",
    "    der[der>0] = 1                      \n",
    "    der[der<0] = 0\n",
    "    \n",
    "    return der"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function which generates the weight tenors used in the network, the weight tensor of the embedding layer and the weight\n",
    "# tensor for the fully connected layer.\n",
    "# vocab_len - is the number of distinct words/tokens in the vocabulary, given a dataset.\n",
    "# dim_embed - the dimension of the embedding space -> dimension of the vector representation of the words.\n",
    "\n",
    "def initializing_weights(vocab_len:int,dim_embed:int)->Dict[str,ndarray]:\n",
    "    W_e = .2*np.random.random((dim_embed,vocab_len)) - .1\n",
    "    v = .2*np.random.random((1,dim_embed)) - .1\n",
    "    b = .1*np.random.randn(1)\n",
    "    \n",
    "    Weights:Dict[str,ndarray] = {'W_e':W_e,'v':v,'b':b}\n",
    "        \n",
    "    return Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Ones(n:int)->ndarray:\n",
    "    Z = np.zeros(n)\n",
    "    \n",
    "    for l in range(n):\n",
    "        Z[l] = 1\n",
    "        \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function computes the outer product between two vectors.\n",
    "def tensor_product_1d(S1:ndarray,S2:ndarray)->ndarray:\n",
    "    assert S1.ndim ==  1 == S2.ndim\n",
    "    Z = np.zeros([S1.shape[0],S2.shape[0]])\n",
    "    \n",
    "    for l in range(Z.shape[0]):   # The output -> 2d tensor of shape (dimension of S1, dimension of S2)\n",
    "        for g in range(Z.shape[1]):\n",
    "            Z[l,g] = S1[l]*S2[g]\n",
    "            \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A neural net with an embedding layer and a fully connected layer which outputs a scalar value.\n",
    "# f2(f1) -> Embedding layer.\n",
    "# f5(f4(f3)) -> fully connected layer.\n",
    "# Technically, a 3 layered network including the virtual layer -> the input layer.\n",
    "# I used tanh -> the hyperbolic tangent as the activation for the embedding layer and Sigmoid for the output layer.\n",
    "# W_e -> the weight filter in the embedding layer.\n",
    "# v -> the weight vector of the dense layer.\n",
    "# b -> the bias term in the dense layer.\n",
    "# input_ -> input data -> as a sequence of lists of lists -> array of array of integers -> a 2d tensor. \n",
    "# output_ -> a list of numbers -> the positive review is mapped into 1, the negative review is mapped into 0.\n",
    "\n",
    "\n",
    "def Embed_Net(input_:ndarray,output_:ndarray,n_iter:int,dim_embed:int,vocab_len:int,l_r:float = .01,seed:int=1)->Dict[str,ndarray]:\n",
    "    \n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    Weight_tensors = initializing_weights(vocab_len,dim_embed)  # Intializing the weight tensors.\n",
    "    \n",
    "    W_e = Weight_tensors['W_e']   # The embedding layer weight tensor.\n",
    "    \n",
    "    v = Weight_tensors['v']  # The dense/fully connected layer weight vector.\n",
    "    \n",
    "    b = Weight_tensors['b']  # The bias term in the fully connected layer.\n",
    "    \n",
    "    num_data = len(input_)  # the number of datapoints in the dataset. In this case there are 50,000 datapoints.\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    for l in range(n_iter):\n",
    "        \n",
    "        for g in range(num_data):\n",
    "            S = input_[g]\n",
    "            y = output_[g]\n",
    "            \n",
    "            f1 = np.sum(W_e[:,S],axis = 1)    # f2(f1) -> the embedding layer -> with an activation function -> Sigmoid.\n",
    "            f2 = tanh(f1)\n",
    "            f3 = dot(v,f2)\n",
    "            f4 = f3 + b        # f5(f4(f3)) -> the fully connected layer with a sigmoid activation.\n",
    "            f5 = Sigmoid(f4)\n",
    "            f6 = np.power(y-f5,2)   # The objective function, given a datapoint.\n",
    "            \n",
    "            # Computing the gradient values.\n",
    "            \n",
    "            partial_der_f5 = 2*(f5-y)\n",
    "            partial_der_f4 = partial_der_f5 * Sigmoid_der(f4)\n",
    "            partial_der_b = partial_der_f4\n",
    "            grad_v = (partial_der_f4*f2).reshape(1,v.shape[1])\n",
    "            grad_f2 = partial_der_f4*v\n",
    "            grad_f1 = (grad_f2*tanh_der(f1)).reshape(-1)\n",
    "            grad_W_e = tensor_product_1d(grad_f1,Create_Ones(W_e[:,S].shape[1]))\n",
    "            \n",
    "            v -= grad_v * l_r\n",
    "            b -= partial_der_b * l_r\n",
    "            W_e[:,S] -= grad_W_e * l_r\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    Output:Dict[str,ndarray] = {'W_e':W_e,'v':v,'b':b}\n",
    "        \n",
    "        \n",
    "    return Output\n",
    "            \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = Embed_Net(input_[:20000],output_[:20000],5,100,vocab_len,.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W_e': array([[-0.0165956 ,  0.04407184, -0.09997713, ...,  0.00463685,\n",
       "         -0.09840342, -0.04059723],\n",
       "        [ 0.01043418,  0.02996012,  0.00912912, ...,  0.05221745,\n",
       "          0.00759985,  0.01616961],\n",
       "        [-0.09310446, -0.08659528, -0.02305649, ...,  0.08856981,\n",
       "         -0.08860365, -0.09315457],\n",
       "        ...,\n",
       "        [-0.06698011,  0.0655957 , -0.01283049, ..., -0.03500108,\n",
       "         -0.04838947,  0.06598333],\n",
       "        [ 0.06453345, -0.067423  , -0.0307061 , ...,  0.00196699,\n",
       "          0.00488216,  0.05662945],\n",
       "        [-0.00265604,  0.0799978 , -0.04438327, ...,  0.06979134,\n",
       "          0.05624882, -0.06717892]]),\n",
       " 'v': array([[-0.00877658, -0.53474038, -0.42072713,  0.42846431,  0.1588675 ,\n",
       "          0.7168066 , -0.28139637,  0.54459042, -0.00607082, -0.3830477 ,\n",
       "          1.19604575,  0.42197763, -0.85353014, -0.53542437,  0.43012448,\n",
       "          0.25172803,  0.22030041,  0.72094483,  0.4210144 , -1.13941277,\n",
       "          0.26043348, -0.32011424,  0.56702754,  0.18438183, -0.07562528,\n",
       "          0.12412173,  0.2117521 ,  0.2852017 , -0.27458049,  0.40418513,\n",
       "          0.79058054, -0.60359993,  0.17592702, -0.31409233, -0.09022566,\n",
       "          0.17339799, -0.00967315,  0.00636417, -0.13330038, -0.15626693,\n",
       "          0.59311551, -0.32430629, -0.18235383, -0.25690867,  0.00660271,\n",
       "         -0.01181097, -0.45491074, -0.66608956, -0.06311249,  0.27769493,\n",
       "         -0.06629521, -0.2031314 , -0.51034766, -0.35150621,  0.10590536,\n",
       "         -0.22699709, -0.12025337, -0.14590766, -0.24062294, -0.36720785,\n",
       "         -0.34215297, -0.04552198, -0.49673797, -0.98066455, -0.19844203,\n",
       "          0.99336803, -0.55071354,  0.22657357, -0.33331696,  0.248437  ,\n",
       "         -0.73709365, -1.41422073, -0.16056047,  0.25820222,  0.19049417,\n",
       "          0.2526805 ,  0.26340237, -0.21854296,  0.70950675, -0.51688099,\n",
       "         -0.32990132, -0.39469053,  0.17090819,  0.22415401,  0.34757048,\n",
       "         -0.40833924,  0.35625843,  0.26731636, -0.02258272, -0.29010756,\n",
       "          0.20629966,  0.01226187,  0.67451921, -0.13610199,  0.27925235,\n",
       "          0.26013477,  0.05692606, -0.11793974, -0.63520707, -0.34325968]]),\n",
       " 'b': array([0.11872859])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_:ndarray,W:Dict[str,ndarray])->ndarray:\n",
    "    W_e = W['W_e']\n",
    "    v = W['v']\n",
    "    b = W['b']\n",
    "    output = []\n",
    "    \n",
    "    for l in range(len(input_)):\n",
    "        S = input_[l]\n",
    "        \n",
    "        f1 = np.sum(W_e[:,S],axis = 1)\n",
    "        f2 = tanh(f1)\n",
    "        f3 = Sigmoid(dot(v,f2) + b)\n",
    "        \n",
    "        if f3 > .5:\n",
    "            output.append(1)\n",
    "        else:\n",
    "            output.append(0)\n",
    "            \n",
    "    return np.array(output)\n",
    "            \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(input_[:20],W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_[:20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classification_Error_rate(y_obs:ndarray,y_pred:ndarray)->float:\n",
    "    assert y_obs.ndim == y_pred.ndim == 1\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for l in range(len(y_pred)):\n",
    "        if y_obs[l] != y_pred[l]:\n",
    "            count+=1\n",
    "        else:\n",
    "            count+=0\n",
    "    return count/len(y_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(input_[30000:40000],W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1488"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Classification_Error_rate(output_[30000:40000],y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy_Score(y_obs:ndarray,y_pred:ndarray)->float:\n",
    "    assert y_obs.ndim == y_pred.ndim == 1\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for l in range(len(y_obs)):\n",
    "        if y_obs[l] == y_pred[l]:\n",
    "            count += 1\n",
    "        else:\n",
    "            count += 0\n",
    "    \n",
    "    return (count/(len(y_pred)))*100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.11999999999999"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy_Score(output_[30000:40000],y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
